{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Base2 Model: Lightweight Text Sentiment Analysis Using PyTorch\n",
    "\n",
    "## Introduction\n",
    "The **Base2 model** improves upon the initial sentiment analysis approach by leveraging a **neural network** with **LSTM** (Long Short-Term Memory) layers, which are more powerful for sequential data like text. Unlike the previous model based on `TextBlob`, this model is designed to handle more complex patterns in text while maintaining **computational efficiency** by using a **lightweight tokenizer** and a **custom vocabulary** built directly from the dataset.\n",
    "\n",
    "### Why Use LSTM?\n",
    "1. **Sequential Learning**: LSTM is ideal for processing text data as it can learn the relationships between words in a sentence and capture the context necessary to understand sentiment.\n",
    "2. **Memory Capabilities**: LSTM can \"remember\" important information across longer sequences, which is particularly useful when analyzing longer text entries.\n",
    "3. **Efficiency with Lightweight Components**: By using a basic tokenizer and a custom vocabulary built from the data, we reduce computational load and memory usage while still benefiting from deep learning performance.\n",
    "\n",
    "### Model Architecture\n",
    "The architecture of this model includes the following layers:\n",
    "1. **Embedding Layer**: We use a randomly initialized embedding layer, which is learned during training. This reduces the complexity compared to pre-trained embeddings (like GloVe) while still providing effective representations.\n",
    "2. **LSTM Layer**: This layer processes the sequential data (text) and learns the underlying patterns that contribute to sentiment.\n",
    "3. **Fully Connected Layer**: This final layer maps the LSTM's output to sentiment categories (positive, negative, neutral).\n",
    "4. **Softmax Activation**: Converts the output to probabilities for each sentiment class.\n",
    "\n",
    "### Key Benefits of the Lightweight Model\n",
    "- **Reduced Complexity**: By eliminating external libraries for embeddings and using a custom tokenizer and vocabulary, the model is faster and uses less memory.\n",
    "- **Performance**: The model retains its ability to capture nuanced sentiments through LSTM while being optimized for GPU acceleration.\n",
    "- **Scalability**: It can be extended to larger datasets or integrated with additional features like personalized music recommendations.\n",
    "\n",
    "## Dataset\n",
    "The dataset consists of labeled text entries (e.g., movie reviews, product reviews, social media posts) with sentiment labels:\n",
    "- **Positive**\n",
    "- **Negative**\n",
    "- **Neutral**\n",
    "\n",
    "The text data is preprocessed by:\n",
    "1. **Tokenization**: Using a basic tokenizer that splits sentences into words based on spaces.\n",
    "2. **Padding and Truncation**: Ensuring all input sequences have the same length.\n",
    "3. **Custom Vocabulary**: A lightweight vocabulary built from the dataset itself, ensuring minimal computational overhead.\n",
    "\n",
    "## Training Process\n",
    "The model is trained on a split dataset (training and testing), where it learns to predict sentiment based on the text input. After each epoch, we evaluate the modelâ€™s performance on the test set using the following metrics:\n",
    "- **Accuracy**: The percentage of correct predictions.\n",
    "- **Loss**: Measures how far the predicted sentiments are from the actual labels, minimized during training.\n",
    "\n",
    "## Music Recommendation System\n",
    "Once the sentiment is predicted, the app can provide personalized music recommendations based on the sentiment:\n",
    "- **Positive Sentiment**: Upbeat and energizing music.\n",
    "- **Negative Sentiment**: Relaxing or comforting music.\n",
    "- **Neutral Sentiment**: Balanced or neutral music.\n",
    "\n",
    "The system can deliver these recommendations in real-time, enhancing user experience based on their detected emotional state.\n",
    "\n",
    "## Next Steps for Improvement\n",
    "1. **Transformer Models**: In future iterations, models like **BERT** or **GPT** could replace LSTM to achieve higher accuracy in sentiment prediction.\n",
    "2. **Multimodal Analysis**: We can combine text-based sentiment analysis with other inputs, such as **facial expressions** or **voice analysis**, to detect emotions more comprehensively.\n",
    "3. **Personalized Recommendations**: The app can learn individual preferences over time and adjust its music recommendations for each user, making the experience more personalized.\n",
    "\n",
    "## Conclusion\n",
    "The **Base2 model** enhances the previous sentiment analysis by utilizing **LSTM** in a lightweight, efficient manner. This model balances **performance** and **efficiency** by using a simpler tokenizer and custom vocabulary while retaining the power of deep learning. It can be further improved and scaled by integrating advanced models or additional input modalities.\n"
   ],
   "id": "2834b22a9ae10855"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:38.609847Z",
     "start_time": "2024-10-13T08:35:36.513279Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:41.370450Z",
     "start_time": "2024-10-13T08:35:39.691813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1d1c96275f2975ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:41.380918Z",
     "start_time": "2024-10-13T08:35:41.374896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dummy data\n",
    "data = [\n",
    "    (\"I love this!\", 1),\n",
    "    (\"This is terrible.\", 0),\n",
    "    (\"I'm feeling great today.\", 1),\n",
    "    (\"Not happy with the product.\", 0),\n",
    "    (\"It was an average experience.\", 2)\n",
    "]"
   ],
   "id": "835329de9d3ef2b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:45.666179Z",
     "start_time": "2024-10-13T08:35:45.658364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic whitespace tokenizer\n",
    "def basic_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build custom vocabulary based on dataset\n",
    "def build_vocab(dataset, tokenizer, max_vocab_size=5000):\n",
    "    word_freq = Counter()\n",
    "    for sentence, _ in dataset:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_freq.update(tokens)\n",
    "    \n",
    "    # Create vocab dict with most common words and assign indices\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(word_freq.most_common(max_vocab_size))}\n",
    "    vocab[\"<pad>\"] = 0  # Padding token\n",
    "    vocab[\"<unk>\"] = 1  # Unknown token\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary from data\n",
    "vocab = build_vocab(data, basic_tokenizer)"
   ],
   "id": "ab84cce417c2da48",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:50.301524Z",
     "start_time": "2024-10-13T08:35:50.295634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset class for PyTorch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, vocab, max_len=50):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        indices = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [self.vocab[\"<pad>\"]] * (self.max_len - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_len]\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ],
   "id": "a5edb914814b98ee",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:35:56.567459Z",
     "start_time": "2024-10-13T08:35:56.554913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset and Dataloader\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_dataset = SentimentDataset(train_data, basic_tokenizer, vocab, max_len=50)\n",
    "test_dataset = SentimentDataset(test_data, basic_tokenizer, vocab, max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ],
   "id": "b261d305ecdaf85b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:36:01.207191Z",
     "start_time": "2024-10-13T08:36:01.193652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model definition\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n"
   ],
   "id": "e201ae5da5c7c024",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:38:43.560195Z",
     "start_time": "2024-10-13T08:38:43.538689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50  # Reduced dimensionality\n",
    "hidden_dim = 128\n",
    "output_dim = 3  # For positive, negative, neutral\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "efedbd8bfb28a517",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:38:44.755666Z",
     "start_time": "2024-10-13T08:38:43.929230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for text, label in train_loader:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss/len(train_loader)}\")"
   ],
   "id": "208112aaaf519e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.0863237380981445\n",
      "Epoch 2: Loss = 1.8107515573501587\n",
      "Epoch 3: Loss = 1.1190261840820312\n",
      "Epoch 4: Loss = 1.089711308479309\n",
      "Epoch 5: Loss = 1.1152374744415283\n",
      "Epoch 6: Loss = 1.099804162979126\n",
      "Epoch 7: Loss = 1.083448886871338\n",
      "Epoch 8: Loss = 1.0722392797470093\n",
      "Epoch 9: Loss = 1.0647034645080566\n",
      "Epoch 10: Loss = 1.059110403060913\n",
      "Epoch 11: Loss = 1.054405689239502\n",
      "Epoch 12: Loss = 1.0501315593719482\n",
      "Epoch 13: Loss = 1.0462766885757446\n",
      "Epoch 14: Loss = 1.0431077480316162\n",
      "Epoch 15: Loss = 1.0409505367279053\n",
      "Epoch 16: Loss = 1.0399620532989502\n",
      "Epoch 17: Loss = 1.0400193929672241\n",
      "Epoch 18: Loss = 1.0407772064208984\n",
      "Epoch 19: Loss = 1.0417776107788086\n",
      "Epoch 20: Loss = 1.04258394241333\n",
      "Epoch 21: Loss = 1.042914628982544\n",
      "Epoch 22: Loss = 1.042709469795227\n",
      "Epoch 23: Loss = 1.0420995950698853\n",
      "Epoch 24: Loss = 1.0413143634796143\n",
      "Epoch 25: Loss = 1.0405813455581665\n",
      "Epoch 26: Loss = 1.0400559902191162\n",
      "Epoch 27: Loss = 1.039795160293579\n",
      "Epoch 28: Loss = 1.039772629737854\n",
      "Epoch 29: Loss = 1.0399129390716553\n",
      "Epoch 30: Loss = 1.0401278734207153\n",
      "Epoch 31: Loss = 1.0403395891189575\n",
      "Epoch 32: Loss = 1.0404927730560303\n",
      "Epoch 33: Loss = 1.0405569076538086\n",
      "Epoch 34: Loss = 1.040527105331421\n",
      "Epoch 35: Loss = 1.0404174327850342\n",
      "Epoch 36: Loss = 1.0402556657791138\n",
      "Epoch 37: Loss = 1.040076494216919\n",
      "Epoch 38: Loss = 1.039913535118103\n",
      "Epoch 39: Loss = 1.039793610572815\n",
      "Epoch 40: Loss = 1.0397312641143799\n",
      "Epoch 41: Loss = 1.039726972579956\n",
      "Epoch 42: Loss = 1.0397683382034302\n",
      "Epoch 43: Loss = 1.039833903312683\n",
      "Epoch 44: Loss = 1.039899468421936\n",
      "Epoch 45: Loss = 1.0399445295333862\n",
      "Epoch 46: Loss = 1.0399576425552368\n",
      "Epoch 47: Loss = 1.0399378538131714\n",
      "Epoch 48: Loss = 1.0398931503295898\n",
      "Epoch 49: Loss = 1.0398370027542114\n",
      "Epoch 50: Loss = 1.0397834777832031\n",
      "Epoch 51: Loss = 1.0397441387176514\n",
      "Epoch 52: Loss = 1.0397237539291382\n",
      "Epoch 53: Loss = 1.0397225618362427\n",
      "Epoch 54: Loss = 1.0397354364395142\n",
      "Epoch 55: Loss = 1.0397549867630005\n",
      "Epoch 56: Loss = 1.039773941040039\n",
      "Epoch 57: Loss = 1.0397865772247314\n",
      "Epoch 58: Loss = 1.0397896766662598\n",
      "Epoch 59: Loss = 1.039783239364624\n",
      "Epoch 60: Loss = 1.0397698879241943\n",
      "Epoch 61: Loss = 1.0397534370422363\n",
      "Epoch 62: Loss = 1.0397381782531738\n",
      "Epoch 63: Loss = 1.0397272109985352\n",
      "Epoch 64: Loss = 1.039721965789795\n",
      "Epoch 65: Loss = 1.0397225618362427\n",
      "Epoch 66: Loss = 1.0397272109985352\n",
      "Epoch 67: Loss = 1.0397331714630127\n",
      "Epoch 68: Loss = 1.0397385358810425\n",
      "Epoch 69: Loss = 1.0397413969039917\n",
      "Epoch 70: Loss = 1.039740800857544\n",
      "Epoch 71: Loss = 1.0397374629974365\n",
      "Epoch 72: Loss = 1.0397326946258545\n",
      "Epoch 73: Loss = 1.0397276878356934\n",
      "Epoch 74: Loss = 1.0397237539291382\n",
      "Epoch 75: Loss = 1.0397217273712158\n",
      "Epoch 76: Loss = 1.0397216081619263\n",
      "Epoch 77: Loss = 1.0397229194641113\n",
      "Epoch 78: Loss = 1.0397248268127441\n",
      "Epoch 79: Loss = 1.0397263765335083\n",
      "Epoch 80: Loss = 1.0397270917892456\n",
      "Epoch 81: Loss = 1.039726972579956\n",
      "Epoch 82: Loss = 1.03972589969635\n",
      "Epoch 83: Loss = 1.0397244691848755\n",
      "Epoch 84: Loss = 1.0397229194641113\n",
      "Epoch 85: Loss = 1.0397218465805054\n",
      "Epoch 86: Loss = 1.0397212505340576\n",
      "Epoch 87: Loss = 1.0397213697433472\n",
      "Epoch 88: Loss = 1.0397218465805054\n",
      "Epoch 89: Loss = 1.0397223234176636\n",
      "Epoch 90: Loss = 1.0397226810455322\n",
      "Epoch 91: Loss = 1.0397226810455322\n",
      "Epoch 92: Loss = 1.0397226810455322\n",
      "Epoch 93: Loss = 1.039722204208374\n",
      "Epoch 94: Loss = 1.0397217273712158\n",
      "Epoch 95: Loss = 1.0397213697433472\n",
      "Epoch 96: Loss = 1.0397210121154785\n",
      "Epoch 97: Loss = 1.0397210121154785\n",
      "Epoch 98: Loss = 1.039721131324768\n",
      "Epoch 99: Loss = 1.0397212505340576\n",
      "Epoch 100: Loss = 1.0397214889526367\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:38:47.163811Z",
     "start_time": "2024-10-13T08:38:47.140071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for text, label in test_loader:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        outputs = model(text)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total}%\")"
   ],
   "id": "d5ba819b835eeadb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0%\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:38:47.833073Z",
     "start_time": "2024-10-13T08:38:47.827904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample Input Sentiment Analysis\n",
    "def analyze_sentiment(text):\n",
    "    tokens = basic_tokenizer(text)\n",
    "    indices = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    if len(indices) < 50:\n",
    "        indices += [vocab[\"<pad>\"]] * (50 - len(indices))\n",
    "    else:\n",
    "        indices = indices[:50]\n",
    "    input_tensor = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n"
   ],
   "id": "270df573133a0003",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T08:38:48.501411Z",
     "start_time": "2024-10-13T08:38:48.487949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing with new input\n",
    "sample_input = \"I'm feeling amazing today!\"\n",
    "sentiment = analyze_sentiment(sample_input)\n",
    "print(f\"Predicted Sentiment: {sentiment}\")  # 1 -> Positive, 0 -> Negative, 2 -> Neutral"
   ],
   "id": "4b15228055d24c60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: 1\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f7c5d8db95569f3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

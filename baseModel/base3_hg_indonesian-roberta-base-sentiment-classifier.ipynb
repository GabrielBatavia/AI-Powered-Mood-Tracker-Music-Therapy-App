{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Indonesian RoBERTa Base Sentiment Classifier\n",
    "\n",
    "The **Indonesian RoBERTa Base Sentiment Classifier** is a sentiment-text-classification model based on the RoBERTa architecture. This model was originally the **Indonesian RoBERTa Base**, which was fine-tuned on indonlu's **SmSA** dataset consisting of Indonesian comments and reviews.\n",
    "\n",
    "### Model Performance\n",
    "The model demonstrates excellent performance with the following evaluation results:\n",
    "- **Accuracy**: 94.36%\n",
    "- **F1-macro**: 92.42%\n",
    "\n",
    "On the **benchmark test set**, the model achieved:\n",
    "- **Accuracy**: 93.2%\n",
    "- **F1-macro**: 91.02%\n",
    "\n",
    "The model was trained using Hugging Face's **Trainer class** from the Transformers library, with **PyTorch** as the backend framework. However, it is also compatible with other frameworks like TensorFlow.\n",
    "\n",
    "### Dataset and Training\n",
    "The model was trained on the **SmSA** dataset, which consists of Indonesian-language comments and reviews. The training process lasted for 5 epochs, and the best model was loaded at the end of training.\n",
    "\n",
    "Some of the training results include:\n",
    "- **Epoch 1**: Accuracy 92.85%, F1-macro 89.85%\n",
    "- **Epoch 5**: Accuracy 94.2%, F1-macro 92.05%\n",
    "\n",
    "### How to Use\n",
    "To use this model, you can leverage the **sentiment-analysis** pipeline from the Transformers library by defining the model and tokenizer with the specified name.\n",
    "\n",
    "This model can be utilized for various sentiment analysis tasks, such as processing product reviews, social media comments, or user feedback in the Indonesian language.\n"
   ],
   "id": "82f2d9bcfd731976"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T06:16:30.273445Z",
     "start_time": "2024-10-19T06:16:29.676483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the model name\n",
    "pretrained_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=pretrained_name,\n",
    "    tokenizer=pretrained_name\n",
    ")\n",
    "\n",
    "# Test the sentiment analysis with an example sentence\n",
    "text = \"Aku benci dia\"\n",
    "result = nlp(text)\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ],
   "id": "99d63350baad253e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\workenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative', 'score': 0.9876623153686523}]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T06:16:37.321236Z",
     "start_time": "2024-10-19T06:16:36.389854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "pretrained_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "model = AutoModel.from_pretrained(pretrained_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name)"
   ],
   "id": "1a5510e0ea9b13f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at w11wo/indonesian-roberta-base-sentiment-classifier and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:07:53.573808Z",
     "start_time": "2024-10-13T12:07:53.566860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Inspect the model architecture and layers\n",
    "print(\"Model Architecture:\\n\", model)"
   ],
   "id": "d570f7ac5bbc54c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      " RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): RobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x RobertaLayer(\n",
      "        (attention): RobertaAttention(\n",
      "          (self): RobertaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): RobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): RobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): RobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): RobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:08:04.767604Z",
     "start_time": "2024-10-13T12:08:04.761409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Inspect the model layers\n",
    "print(\"\\nModel Layers:\")\n",
    "for idx, layer in enumerate(model.encoder.layer):\n",
    "    print(f\"Layer {idx + 1}: {layer}\")"
   ],
   "id": "c1b22819cac010e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Layers:\n",
      "Layer 1: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 2: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 3: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 4: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 5: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 6: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 7: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 8: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 9: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 10: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 11: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Layer 12: RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:08:22.567391Z",
     "start_time": "2024-10-13T12:08:22.560579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Check the model's named parameters (weights and biases)\n",
    "print(\"\\nModel Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name} | Size: {param.size()}\")"
   ],
   "id": "42e6c501577dade6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Parameters:\n",
      "Parameter: embeddings.word_embeddings.weight | Size: torch.Size([50265, 768])\n",
      "Parameter: embeddings.position_embeddings.weight | Size: torch.Size([514, 768])\n",
      "Parameter: embeddings.token_type_embeddings.weight | Size: torch.Size([1, 768])\n",
      "Parameter: embeddings.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: embeddings.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.0.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.0.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.0.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.0.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.0.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.0.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.0.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.1.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.1.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.1.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.1.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.1.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.1.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.1.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.1.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.2.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.2.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.2.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.2.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.2.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.2.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.2.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.2.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.3.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.3.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.3.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.3.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.3.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.3.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.3.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.3.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.4.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.4.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.4.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.4.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.4.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.4.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.4.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.4.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.5.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.5.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.5.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.5.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.5.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.5.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.5.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.5.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.6.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.6.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.6.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.6.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.6.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.6.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.6.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.6.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.7.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.7.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.7.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.7.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.7.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.7.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.7.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.7.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.8.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.8.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.8.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.8.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.8.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.8.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.8.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.8.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.9.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.9.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.9.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.9.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.9.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.9.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.9.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.9.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.10.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.10.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.10.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.10.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.10.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.10.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.10.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.10.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.self.query.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.11.attention.self.query.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.self.key.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.11.attention.self.key.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.self.value.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.11.attention.self.value.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.output.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: encoder.layer.11.attention.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.attention.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.intermediate.dense.weight | Size: torch.Size([3072, 768])\n",
      "Parameter: encoder.layer.11.intermediate.dense.bias | Size: torch.Size([3072])\n",
      "Parameter: encoder.layer.11.output.dense.weight | Size: torch.Size([768, 3072])\n",
      "Parameter: encoder.layer.11.output.dense.bias | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.output.LayerNorm.weight | Size: torch.Size([768])\n",
      "Parameter: encoder.layer.11.output.LayerNorm.bias | Size: torch.Size([768])\n",
      "Parameter: pooler.dense.weight | Size: torch.Size([768, 768])\n",
      "Parameter: pooler.dense.bias | Size: torch.Size([768])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:08:33.464605Z",
     "start_time": "2024-10-13T12:08:33.460997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Get tokenizer information (useful to understand how the model tokenizes input text)\n",
    "print(\"\\nTokenizer Information:\\n\", tokenizer)"
   ],
   "id": "a76cefabf0405f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer Information:\n",
      " RobertaTokenizerFast(name_or_path='w11wo/indonesian-roberta-base-sentiment-classifier', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T12:08:43.165750Z",
     "start_time": "2024-10-13T12:08:43.161221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Check the model's total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal number of model parameters: {total_params}\")"
   ],
   "id": "cab413d877819e66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of model parameters: 124645632\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "780d3e771448e50b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
